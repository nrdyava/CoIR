{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa5c2087-cef9-4bd4-ae48-24e23dfe3017",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 0\n",
    "results_name = 'CLIP-ViT-B-32'\n",
    "clip_model = 'CLIP-ViT-B/32' # Options: ['CLIP-ViT-B/16', 'CLIP-ViT-B/32', 'CLIP-ViT-L/14', 'CLIP-ViT-L/14@336']\n",
    "batch_size=64\n",
    "\n",
    "out_dir = '/proj/vondrick4/naveen/coir-ret-results'\n",
    "dataset_split = 'test'\n",
    "lasco_data_path = '/proj/vondrick4/naveen/coir-data/MSCOCO_5k'\n",
    "device_map = 'cuda:{}'.format(device)\n",
    "\n",
    "\n",
    "clip_checkpoints = {\n",
    "    'CLIP-ViT-B/16': '/local/vondrick/naveen/pretrained_models/clip/clip-vit-base-patch16',\n",
    "    'CLIP-ViT-B/32': '/local/vondrick/naveen/pretrained_models/clip/clip-vit-base-patch32',\n",
    "    'CLIP-ViT-L/14': '/local/vondrick/naveen/pretrained_models/clip/clip-vit-large-patch14',\n",
    "    'CLIP-ViT-L/14@336': '/local/vondrick/naveen/pretrained_models/clip/clip-vit-large-patch14-336'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7634aa1b-de16-4a99-9bd5-4b681e814b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/proj/vondrick4/naveen/CoIR')\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import CLIPVisionModelWithProjection, CLIPTextModelWithProjection\n",
    "import faiss\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "\n",
    "from src.datasets.mscoco_5k.mscoco_5k_corpus_dataset import mscoco_5k_image_corpus_dataset_clip, mscoco_5k_text_corpus_dataset_clip\n",
    "from src.datasets.mscoco_5k.mscoco_5k_retrieval_dataset import mscoco_5k_retrieval_dataset_clip\n",
    "from src.metrics.metrics import calculate_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44159ec9-8ae3-4ad0-bca3-5d71f0200b6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "032e9c18-dc5f-4a09-9f24-cc32c95efc9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(os.path.join(out_dir, results_name), exist_ok=True)\n",
    "clip_checkpoint_path = clip_checkpoints[clip_model]\n",
    "print('Using device: {}'.format(device_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afff5cbe-7296-4eb2-8bf9-b0632f3913bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4546492b-bb99-46ea-b19a-c3b09ed21daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n"
     ]
    }
   ],
   "source": [
    "image_encoder = CLIPVisionModelWithProjection.from_pretrained(pretrained_model_name_or_path=clip_checkpoint_path, local_files_only=True).to(device)\n",
    "text_encoder = CLIPTextModelWithProjection.from_pretrained(pretrained_model_name_or_path=clip_checkpoint_path, local_files_only=True).to(device)\n",
    "\n",
    "image_encoder.eval()\n",
    "text_encoder.eval()\n",
    "print('Model loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b6df60-d3ec-4124-9850-abe84d305597",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "706d43c3-81c9-4c8c-a0a3-c1d4a1eae61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = image_encoder.config.projection_dim\n",
    "image_index = faiss.IndexFlatIP(d)\n",
    "text_index = faiss.IndexFlatIP(d)\n",
    "\n",
    "res = faiss.StandardGpuResources()\n",
    "image_index = faiss.index_cpu_to_gpu(res, device, image_index)\n",
    "text_index = faiss.index_cpu_to_gpu(res, device, text_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187f05d7-8cce-4f0e-9799-db90d6f9ed16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f82980bc-1d7e-4130-b831-3e286b9cce6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_dataset_image = mscoco_5k_image_corpus_dataset_clip(dataset_split, lasco_data_path, clip_checkpoint_path)\n",
    "corpus_dataloader_image = DataLoader(\n",
    "    dataset=corpus_dataset_image,\n",
    "    collate_fn=corpus_dataset_image.collate_fn,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=10,\n",
    "    pin_memory=True,\n",
    "    drop_last=False,\n",
    "    persistent_workers=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee8bf2c9-3da9-499b-8740-efdead258ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_dataset_text = mscoco_5k_text_corpus_dataset_clip(dataset_split, lasco_data_path, clip_checkpoint_path)\n",
    "corpus_dataloader_text = DataLoader(\n",
    "    dataset=corpus_dataset_text,\n",
    "    collate_fn=corpus_dataset_text.collate_fn,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=10,\n",
    "    pin_memory=True,\n",
    "    drop_last=False,\n",
    "    persistent_workers=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8edc9fcb-9a7e-4faa-bde5-2936e23ec803",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_dataset = mscoco_5k_retrieval_dataset_clip(dataset_split, lasco_data_path, clip_checkpoint_path)\n",
    "retrieval_dataloader = DataLoader(\n",
    "    dataset=retrieval_dataset,\n",
    "    collate_fn=retrieval_dataset.collate_fn,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=10,\n",
    "    pin_memory=True,\n",
    "    drop_last=False,\n",
    "    persistent_workers=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554e0a66-3414-4076-9823-fbc350495f7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e13d377-aa74-434a-850f-4dab246ce3dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing Corpus: IMAGES: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:06<00:00, 12.28it/s]\n"
     ]
    }
   ],
   "source": [
    "index_cntr = 0\n",
    "index_id_to_image_id_map = {}\n",
    "\n",
    "for batch_idx, batch in enumerate(tqdm(corpus_dataloader_image, desc=\"Indexing Corpus: IMAGES\")):\n",
    "    with torch.no_grad():\n",
    "        batch['image']['pixel_values'] = batch['image']['pixel_values'].to(device_map)\n",
    "        image_embeds = image_encoder(**batch['image']).image_embeds\n",
    "        image_embeds = image_embeds / torch.linalg.vector_norm(image_embeds, ord=2, dim=1,keepdim=True)\n",
    "    \n",
    "    image_index.add(image_embeds.cpu())\n",
    "\n",
    "    batch_len = len(batch['image-key'])\n",
    "    batch_start_indx = index_cntr\n",
    "    batch_end_indx = batch_start_indx + batch_len\n",
    "\n",
    "    for key, value in zip(list(range(batch_start_indx, batch_end_indx)), batch['image-key']):\n",
    "        index_id_to_image_id_map[key] = value\n",
    "    index_cntr += batch_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3582a85-bedc-4543-86ca-d76e8ab93528",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing Corpus: TEXT: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:01<00:00, 45.09it/s]\n"
     ]
    }
   ],
   "source": [
    "index_cntr = 0\n",
    "index_id_to_text_id_map = {}\n",
    "\n",
    "for batch_idx, batch in enumerate(tqdm(corpus_dataloader_text, desc=\"Indexing Corpus: TEXT\")):\n",
    "    with torch.no_grad():\n",
    "        batch['text']['input_ids'] = batch['text']['input_ids'].to(device_map)\n",
    "        batch['text']['attention_mask'] = batch['text']['attention_mask'].to(device_map)\n",
    "\n",
    "        text_embeds = text_encoder(**batch['text']).text_embeds\n",
    "        text_embeds = text_embeds / torch.linalg.vector_norm(text_embeds, ord=2, dim=1,keepdim=True)\n",
    "    \n",
    "    text_index.add(text_embeds.cpu())\n",
    "\n",
    "    batch_len = len(batch['text-key'])\n",
    "    batch_start_indx = index_cntr\n",
    "    batch_end_indx = batch_start_indx + batch_len\n",
    "\n",
    "    for key, value in zip(list(range(batch_start_indx, batch_end_indx)), batch['text-key']):\n",
    "        index_id_to_text_id_map[key] = value\n",
    "    index_cntr += batch_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b47f77d-f779-4d5b-9722-9ffb8ceffcb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac4ff2c8-3845-438a-a390-a9cb2cb45f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_func_image = np.vectorize(lambda x: index_id_to_image_id_map[x])\n",
    "map_func_text = np.vectorize(lambda x: index_id_to_text_id_map[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffe1933-3f48-4d47-8122-46967d7e3f40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "99e91c2f-e29d-4a73-a288-97b5a796f55f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieval Task: IMG-2-TEXT: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 625/625 [00:07<00:00, 80.29it/s]\n"
     ]
    }
   ],
   "source": [
    "output_img_2_text = []\n",
    "\n",
    "retrieval_dataset = mscoco_5k_retrieval_dataset_clip(dataset_split, lasco_data_path, clip_checkpoint_path)\n",
    "retrieval_dataloader = DataLoader(\n",
    "    dataset=retrieval_dataset,\n",
    "    collate_fn=retrieval_dataset.collate_fn,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=10,\n",
    "    pin_memory=True,\n",
    "    drop_last=False,\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "for batch_idx, batch in enumerate(tqdm(retrieval_dataloader , desc=\"Retrieval Task: IMG-2-TEXT\")):\n",
    "    with torch.no_grad():\n",
    "        batch['image']['pixel_values'] = batch['image']['pixel_values'].to(device_map)\n",
    "        image_embeds = image_encoder(**batch['image']).image_embeds\n",
    "        image_embeds = image_embeds / torch.linalg.vector_norm(image_embeds, ord=2, dim=1, keepdim=True)\n",
    "\n",
    "    D, I = text_index.search(image_embeds.cpu(), k=1000)\n",
    "    I = map_func_text(I)\n",
    "\n",
    "    batch_size = len(batch['image-id'])\n",
    "    for i in range(batch_size):\n",
    "        output_img_2_text.append({\n",
    "            'id': batch['id'][i],\n",
    "            'image-id': batch['image-id'][i],\n",
    "            'text-id': batch['text-id'][i],\n",
    "            'text-raw': batch['text-raw'][i],\n",
    "            'top_1000_ret_cands': I[i][:].tolist(),\n",
    "            'top_1000_ret_cands_cos_sims': D[i][:].tolist()\n",
    "            })\n",
    "\n",
    "with open(os.path.join(out_dir, results_name, 'outputs'+'-mscoco-5k-[image-2-text]-'+dataset_split+'.json'), \"w\") as json_file:\n",
    "    json.dump(output_img_2_text, json_file, indent=4)\n",
    "\n",
    "\n",
    "metrics = []\n",
    "ground_truths = np.array(list(map(lambda x: x['text-id'], output_img_2_text)))\n",
    "retrieved_candidates = np.array(list(map(lambda x: x['top_1000_ret_cands'], output_img_2_text)))\n",
    "\n",
    "metrics.append({\"Recall@1\": 100*calculate_recall(ground_truths, retrieved_candidates, 1)})\n",
    "metrics.append({\"Recall@5\": 100*calculate_recall(ground_truths, retrieved_candidates, 5)})\n",
    "metrics.append({\"Recall@10\": 100*calculate_recall(ground_truths, retrieved_candidates, 10)})\n",
    "metrics.append({\"Recall@50\": 100*calculate_recall(ground_truths, retrieved_candidates, 50)})\n",
    "metrics.append({\"Recall@100\": 100*calculate_recall(ground_truths, retrieved_candidates, 100)})\n",
    "metrics.append({\"Recall@500\": 100*calculate_recall(ground_truths, retrieved_candidates, 500)})\n",
    "metrics.append({\"Recall@1000\": 100*calculate_recall(ground_truths, retrieved_candidates, 1000)})\n",
    "\n",
    "with open(os.path.join(out_dir, results_name, 'metrics'+'-mscoco-5k-[image-2-text]-'+dataset_split+'.json'), \"w\") as json_file:\n",
    "    json.dump(metrics, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3a4143-c90e-4a6d-b5d8-4d0f0860b740",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2ef37d03-c9ab-4414-99ab-95f428d25b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieval Task: IMG-2-TEXT: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 625/625 [00:07<00:00, 82.49it/s]\n"
     ]
    }
   ],
   "source": [
    "output_text_2_img = []\n",
    "\n",
    "retrieval_dataset = mscoco_5k_retrieval_dataset_clip(dataset_split, lasco_data_path, clip_checkpoint_path)\n",
    "retrieval_dataloader = DataLoader(\n",
    "    dataset=retrieval_dataset,\n",
    "    collate_fn=retrieval_dataset.collate_fn,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=10,\n",
    "    pin_memory=True,\n",
    "    drop_last=False,\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "for batch_idx, batch in enumerate(tqdm(retrieval_dataloader , desc=\"Retrieval Task: IMG-2-TEXT\")):\n",
    "    with torch.no_grad():\n",
    "        batch['text']['input_ids'] = batch['text']['input_ids'].to(device_map)\n",
    "        batch['text']['attention_mask'] = batch['text']['attention_mask'].to(device_map)\n",
    "        \n",
    "        text_embeds = text_encoder(**batch['text']).text_embeds\n",
    "        text_embeds = text_embeds / torch.linalg.vector_norm(text_embeds, ord=2, dim=1, keepdim=True)\n",
    "\n",
    "    D, I = image_index.search(text_embeds.cpu(), k=1000)\n",
    "    I = map_func_image(I)\n",
    "\n",
    "    batch_size = len(batch['text-id'])\n",
    "    for i in range(batch_size):\n",
    "        output_text_2_img.append({\n",
    "            'id': batch['id'][i],\n",
    "            'image-id': batch['image-id'][i],\n",
    "            'text-id': batch['text-id'][i],\n",
    "            'text-raw': batch['text-raw'][i],\n",
    "            'top_1000_ret_cands': I[i][:].tolist(),\n",
    "            'top_1000_ret_cands_cos_sims': D[i][:].tolist()\n",
    "            })\n",
    "\n",
    "with open(os.path.join(out_dir, results_name, 'outputs'+'-mscoco-5k-[text-2-image]-'+dataset_split+'.json'), \"w\") as json_file:\n",
    "    json.dump(output_text_2_img, json_file, indent=4)\n",
    "\n",
    "\n",
    "metrics = []\n",
    "ground_truths = np.array(list(map(lambda x: x['image-id'], output_text_2_img)))\n",
    "retrieved_candidates = np.array(list(map(lambda x: x['top_1000_ret_cands'], output_text_2_img)))\n",
    "\n",
    "metrics.append({\"Recall@1\": 100*calculate_recall(ground_truths, retrieved_candidates, 1)})\n",
    "metrics.append({\"Recall@5\": 100*calculate_recall(ground_truths, retrieved_candidates, 5)})\n",
    "metrics.append({\"Recall@10\": 100*calculate_recall(ground_truths, retrieved_candidates, 10)})\n",
    "metrics.append({\"Recall@50\": 100*calculate_recall(ground_truths, retrieved_candidates, 50)})\n",
    "metrics.append({\"Recall@100\": 100*calculate_recall(ground_truths, retrieved_candidates, 100)})\n",
    "metrics.append({\"Recall@500\": 100*calculate_recall(ground_truths, retrieved_candidates, 500)})\n",
    "metrics.append({\"Recall@1000\": 100*calculate_recall(ground_truths, retrieved_candidates, 1000)})\n",
    "\n",
    "with open(os.path.join(out_dir, results_name, 'metrics'+'-mscoco-5k-[text-2-image]-'+dataset_split+'.json'), \"w\") as json_file:\n",
    "    json.dump(metrics, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9c15f1-0aa1-4e12-9228-d8668d7d7fee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6c43e13c-ff66-475a-aae5-fa42e5fccdf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Recall@1': 29.459999999999997},\n",
       " {'Recall@5': 54.2},\n",
       " {'Recall@10': 65.16},\n",
       " {'Recall@50': 88.98},\n",
       " {'Recall@100': 96.17999999999999},\n",
       " {'Recall@500': 99.42},\n",
       " {'Recall@1000': 99.78}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648c8de4-d5a7-464b-a649-a3a58b18e358",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8160ad2e-dde1-4e47-8128-4264d55858c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
