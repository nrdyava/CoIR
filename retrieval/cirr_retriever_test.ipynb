{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e4457c2-7115-4e6d-9fa2-a5c71555de51",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 0\n",
    "results_name = 'CLIP-ViT-B/32'\n",
    "clip_model = 'CLIP-ViT-B/32' # Options: ['CLIP-ViT-B/16', 'CLIP-ViT-B/32', 'CLIP-ViT-L/14', 'CLIP-ViT-L/14@336']\n",
    "batch_size=64\n",
    "\n",
    "out_dir = '/proj/vondrick4/naveen/coir-ret-results'\n",
    "dataset_split = 'val'\n",
    "lasco_data_path = '/local/vondrick/naveen/coir-data/CIRR'\n",
    "device_map = 'cuda:{}'.format(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba470851-3e07-4dd4-9fa6-d4d08e4d73aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_checkpoints = {\n",
    "    'CLIP-ViT-B/16': '/local/vondrick/naveen/pretrained_models/clip/clip-vit-base-patch16',\n",
    "    'CLIP-ViT-B/32': '/local/vondrick/naveen/pretrained_models/clip/clip-vit-base-patch32',\n",
    "    'CLIP-ViT-L/14': '/local/vondrick/naveen/pretrained_models/clip/clip-vit-large-patch14',\n",
    "    'CLIP-ViT-L/14@336': '/local/vondrick/naveen/pretrained_models/clip/clip-vit-large-patch14-336'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d071a86-b0d6-4c2c-9ef0-790a7edf1608",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/proj/vondrick4/naveen/CoIR')\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import CLIPVisionModelWithProjection, CLIPTextModelWithProjection\n",
    "import faiss\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "\n",
    "from src.datasets.cirr.cirr_corpus_dataset import cirr_corpus_dataset_clip\n",
    "from src.datasets.cirr.cirr_retrieval_dataset import cirr_retrieval_dataset_clip\n",
    "from src.metrics.metrics import calculate_recall, calculate_APs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d8faf0-4029-41eb-ac54-29b8543a3638",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9c48619-0eb0-4ac3-8340-2e5ede3a131e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(os.path.join(out_dir, results_name), exist_ok=True)\n",
    "clip_checkpoint_path = clip_checkpoints[clip_model]\n",
    "print('Using device: {}'.format(device_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e23912-eae2-405d-b993-d3c85a2a5ff1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07a99c10-a779-41a2-8f98-379317e8df16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n"
     ]
    }
   ],
   "source": [
    "image_encoder = CLIPVisionModelWithProjection.from_pretrained(pretrained_model_name_or_path=clip_checkpoint_path, local_files_only=True).to(device)\n",
    "text_encoder = CLIPTextModelWithProjection.from_pretrained(pretrained_model_name_or_path=clip_checkpoint_path, local_files_only=True).to(device)\n",
    "\n",
    "image_encoder.eval()\n",
    "text_encoder.eval()\n",
    "print('Model loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1b86ca-c624-4b3b-974b-267d83a1b4e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea08e5f5-6efb-4622-ad36-c4d2e1009b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = image_encoder.config.projection_dim\n",
    "index = faiss.IndexFlatIP(d)\n",
    "\n",
    "res = faiss.StandardGpuResources()\n",
    "index = faiss.index_cpu_to_gpu(res, device, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2daa821a-f325-4c9e-b464-602525891147",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d1f83fb-6ad4-43ca-a46a-c2533acd3db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_dataset = cirr_corpus_dataset_clip(dataset_split, lasco_data_path, clip_checkpoint_path)\n",
    "corpus_dataloader = DataLoader(\n",
    "    dataset=corpus_dataset,\n",
    "    collate_fn=corpus_dataset.collate_fn,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=10,\n",
    "    pin_memory=True,\n",
    "    drop_last=False,\n",
    "    persistent_workers=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b073f08-5165-4930-a031-c125482f4747",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_dataset = cirr_retrieval_dataset_clip(dataset_split, lasco_data_path, clip_checkpoint_path)\n",
    "retrieval_dataloader = DataLoader(\n",
    "    dataset=retrieval_dataset,\n",
    "    collate_fn=retrieval_dataset.collate_fn,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=10,\n",
    "    pin_memory=True,\n",
    "    drop_last=False,\n",
    "    persistent_workers=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f95e25-d385-4f3f-a736-352a6fc82607",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9424e5d-2e00-4cc6-b337-9f5574cb415b",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_cntr = 0\n",
    "index_id_to_image_id_map = {}\n",
    "image_id_to_index_id_map = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9641543c-f832-4009-bf09-4843402d1853",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af9df0bb-0614-47d2-98eb-a1acf389b64e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing Corpus: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 36/36 [00:06<00:00,  5.34it/s]\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, batch in enumerate(tqdm(corpus_dataloader, desc=\"Indexing Corpus\")):\n",
    "    with torch.no_grad():\n",
    "        batch['image']['pixel_values'] = batch['image']['pixel_values'].to(device_map)\n",
    "        image_embeds = image_encoder(**batch['image']).image_embeds\n",
    "        image_embeds = image_embeds / torch.linalg.vector_norm(image_embeds, ord=2, dim=1,keepdim=True)\n",
    "    \n",
    "    index.add(image_embeds.cpu())\n",
    "\n",
    "    batch_len = len(batch['image-key'])\n",
    "    batch_start_indx = index_cntr\n",
    "    batch_end_indx = batch_start_indx + batch_len\n",
    "\n",
    "    for key, value in zip(list(range(batch_start_indx, batch_end_indx)), batch['image-key']):\n",
    "        index_id_to_image_id_map[key] = value\n",
    "        image_id_to_index_id_map[value] = key\n",
    "    index_cntr += batch_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b30fa8d-aeeb-41c7-b9f2-71a51fb2aca5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18190b0f-7bd7-43d5-ac89-d08d25712e48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2287933b-b5ac-4f48-83c1-3fea5281d12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_func = np.vectorize(lambda x: index_id_to_image_id_map[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f05abc06-6a0c-4f65-9bef-85910dc2d73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = []\n",
    "output_light = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378b5856-d6e7-46f3-8440-9b0e88eabd8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d051ea56-302e-4a9e-8c79-e48921d06f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieval Task: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4181/4181 [01:02<00:00, 67.13it/s]\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, batch in enumerate(tqdm(retrieval_dataloader , desc=\"Retrieval Task\")):\n",
    "    with torch.no_grad():\n",
    "        batch['query-image']['pixel_values'] = batch['query-image']['pixel_values'].to(device_map)\n",
    "        batch['query-text']['input_ids'] = batch['query-text']['input_ids'].to(device_map)\n",
    "        batch['query-text']['attention_mask'] = batch['query-text']['attention_mask'].to(device_map)\n",
    "        \n",
    "        query_image_embeds = image_encoder(**batch['query-image']).image_embeds\n",
    "        query_text_embeds = text_encoder(**batch['query-text']).text_embeds\n",
    "        \n",
    "    target_hat_embeds = query_image_embeds + query_text_embeds\n",
    "    target_hat_embeds = target_hat_embeds / torch.linalg.vector_norm(target_hat_embeds, ord=2, dim=1, keepdim=True)\n",
    "\n",
    "    target_hat_embeds_numpy = target_hat_embeds.cpu().numpy().reshape(-1)\n",
    "    subset_preds = list(map(lambda x: x[0], sorted(list(map(lambda x: (x, np.dot(index.reconstruct(image_id_to_index_id_map[x]), target_hat_embeds_numpy)), batch['subset'][0])), key=lambda x: x[1], reverse=True)))\n",
    "    \n",
    "    D, I = index.search(target_hat_embeds.cpu(), k=1000)\n",
    "    I = map_func(I)\n",
    "\n",
    "    batch_size = len(batch['query-image-id'])\n",
    "    for i in range(batch_size):\n",
    "        output.append({\n",
    "            'id': batch['id'][i],\n",
    "            'query-image-id': batch['query-image-id'][i],\n",
    "            'target-image-id': batch['target-image-id'][i],\n",
    "            'query-text-raw': batch['query-text-raw'][i],\n",
    "            'subset_preds': subset_preds,\n",
    "            'top_1000_ret_cands': I[i][:].tolist(),\n",
    "            'top_1000_ret_cands_cos_sims': D[i][:].tolist()\n",
    "            })\n",
    "        \n",
    "        output_light.append({\n",
    "            'id': batch['id'][i],\n",
    "            'query-image-id': batch['query-image-id'][i],\n",
    "            'target-image-id': batch['target-image-id'][i],\n",
    "            'query-text-raw': batch['query-text-raw'][i],\n",
    "            'subset_preds': subset_preds,\n",
    "            'top_50_ret_cands': I[i][:].tolist()[:50],\n",
    "            'top_50_ret_cands_cos_sims': D[i][:].tolist()[:50]\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33e6d3c-3d2c-476a-8fee-77f958c53ce3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb1bee0-af86-4f69-8a96-dc6b3b0b14cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(out_dir, results_name, f'CIRR_outputs'+'.json'), \"w\") as json_file:\n",
    "    json.dump(output, json_file, indent=4)\n",
    "with open(os.path.join(out_dir, results_name, f'CIRR_outputs_[light]'+'.json'), \"w\") as json_file:\n",
    "    json.dump(output_light, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f639dc6d-58fc-491a-becb-ebd58edea735",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "135dd108-fbd4-469d-97a7-9fa20b625aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = []\n",
    "ground_truths = np.array(list(map(lambda x: x['target-image-id'], output)))\n",
    "retrieved_candidates = np.array(list(map(lambda x: x['top_1000_ret_cands'], output)))\n",
    "retrieved_candidates_subset = np.array(list(map(lambda x: x['subset_preds'], output)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22384f11-c3b5-4b87-a934-8584f4427254",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.append({\"Recall@1\": 100*calculate_recall(ground_truths, retrieved_candidates, 1)})\n",
    "metrics.append({\"Recall@5\": 100*calculate_recall(ground_truths, retrieved_candidates, 5)})\n",
    "metrics.append({\"Recall@10\": 100*calculate_recall(ground_truths, retrieved_candidates, 10)})\n",
    "metrics.append({\"Recall@50\": 100*calculate_recall(ground_truths, retrieved_candidates, 50)})\n",
    "metrics.append({\"Recall@100\": 100*calculate_recall(ground_truths, retrieved_candidates, 100)})\n",
    "metrics.append({\"Recall@500\": 100*calculate_recall(ground_truths, retrieved_candidates, 500)})\n",
    "metrics.append({\"Recall@1000\": 100*calculate_recall(ground_truths, retrieved_candidates, 1000)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aea6b405-abc9-4e23-b499-631f593bf663",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.append({\"Recall_Subset@1\": 100*calculate_recall(ground_truths, retrieved_candidates_subset, 1)})\n",
    "metrics.append({\"Recall_Subset@2\": 100*calculate_recall(ground_truths, retrieved_candidates_subset, 2)})\n",
    "metrics.append({\"Recall_Subset@3\": 100*calculate_recall(ground_truths, retrieved_candidates_subset, 3)})\n",
    "metrics.append({\"Recall_Subset@4\": 100*calculate_recall(ground_truths, retrieved_candidates_subset, 4)})\n",
    "metrics.append({\"Recall_Subset@5\": 100*calculate_recall(ground_truths, retrieved_candidates_subset, 5)})\n",
    "metrics.append({\"Recall_Subset@6\": 100*calculate_recall(ground_truths, retrieved_candidates_subset, 6)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ded5eb8c-fc02-4f55-a892-abbfcf38573d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Recall@1': 0.0},\n",
       " {'Recall@5': 30.471179143745513},\n",
       " {'Recall@10': 45.491509208323365},\n",
       " {'Recall@50': 75.72351112174121},\n",
       " {'Recall@100': 84.69265725902893},\n",
       " {'Recall@500': 97.32121502033007},\n",
       " {'Recall@1000': 99.25855058598422},\n",
       " {'Recall_Subset@1': 0.0},\n",
       " {'Recall_Subset@2': 30.782109543171487},\n",
       " {'Recall_Subset@3': 55.44128198995456},\n",
       " {'Recall_Subset@4': 73.40349198756279},\n",
       " {'Recall_Subset@5': 88.13680937574743},\n",
       " {'Recall_Subset@6': 100.0}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(os.path.join(out_dir, results_name, f'CIRR_metrics'+'.json'), \"w\") as json_file:\n",
    "    json.dump(metrics, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2b0930-b980-4d66-a846-4347804ef869",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef592504-1559-4433-bd01-36f79ba35c34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
