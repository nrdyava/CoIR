{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65732288-9632-4dfc-8ec0-8ad5f6114b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/proj/vondrick2/naveen/miniconda3/envs/tr-vllm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-11-06 08:50:49,741\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "import tqdm as notebook_tqdm\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from huggingface_hub import login\n",
    "\n",
    "with open('/home/nd2794/keys.json', 'r') as file:\n",
    "    keys = json.load(file)\n",
    "\n",
    "cache_dir = '/proj/vondrick4/naveen/HF_CACHE_DIR'\n",
    "hf_token = keys[\"HF_READ_TOKEN\"]\n",
    "login(token=hf_token)\n",
    "\n",
    "os.environ['TRANSFORMERS_CACHE'] = cache_dir\n",
    "os.environ['HUGGINGFACE_HUB_CACHE'] = cache_dir\n",
    "os.environ['PYTORCH_TRANSFORMERS_CACHE'] = cache_dir\n",
    "os.environ['PYTORCH_PRETRAINED_BERT_CACHE'] = cache_dir\n",
    "os.environ['HF_HUB_CACHE'] = cache_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84525582-7feb-4a6f-9cea-1b4aaf3b0058",
   "metadata": {},
   "source": [
    "## Define Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "580c0016-9659-4932-a87a-0e6ac1209f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_instruction(comp, text_event):\n",
    "    comp_ = comp[:-1]\n",
    "\n",
    "    common_start_before = 'You are give an event desciribed by a text. You have to think of exactly 10 possible events that could start before the event.\\n'\n",
    "    common_start_after = 'You are give an event desciribed by a text. You have to think of exactly 10 possible events that could start after the event.\\n'\n",
    "    common_end_before = 'You are give an event desciribed by a text. You have to think of exactly 10 possible events that could end before the event.\\n'\n",
    "    common_end_after = 'You are give an event desciribed by a text. You have to think of exactly 10 possible events that could end after the event.\\n'\n",
    "\n",
    "    json_instruction = \"\"\"Your output should be in standard JSON format only. \n",
    "    There should be no text before or after the JSON.\n",
    "    Your output format should be strictly in the following format:\n",
    "    {\n",
    "        possible_events: [\"event-1\", \"event-2\",.....]\n",
    "    }\\n\\n\n",
    "    \"\"\"\n",
    "\n",
    "    event_prompt = 'EVENT: {}'.format(text_event)\n",
    "    \n",
    "    if comp_ == 'start':\n",
    "        before_instruction = common_start_before + json_instruction + event_prompt\n",
    "        after_instruction = common_start_after + json_instruction + event_prompt\n",
    "    elif comp_ == 'end':\n",
    "        before_instruction = common_end_before + json_instruction + event_prompt\n",
    "        after_instruction = common_end_after + json_instruction + event_prompt\n",
    "        \n",
    "    return {'before': before_instruction, 'after': after_instruction}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ac60ca6-cc93-4f87-aead-c71e0b42eee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_llm_outs(llm_outs):\n",
    "    match = re.search(r'{.*}', llm_outs, re.DOTALL)\n",
    "    global unclear\n",
    "\n",
    "    if match:\n",
    "        answer_json_str = match.group(0)\n",
    "        try:\n",
    "            json_data = json.loads(answer_json_str)\n",
    "            possible_events = json_data['possible_events']\n",
    "        except:\n",
    "            unclear+=1\n",
    "            possible_events = ['An event is shown in the video.']\n",
    "    else:\n",
    "        unclear += 1\n",
    "        possible_events = ['An event is shown in the video.']\n",
    "        \n",
    "    return possible_events"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b487671b-44e0-4ee6-95f9-d7caed4d3e80",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92d94249-59ec-49b5-9550-d4435e97630e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-06 08:51:01 config.py:905] Defaulting to use mp for distributed inference\n",
      "WARNING 11-06 08:51:01 arg_utils.py:957] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.\n",
      "INFO 11-06 08:51:01 config.py:1021] Chunked prefill is enabled with max_num_batched_tokens=512.\n",
      "INFO 11-06 08:51:01 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir='/proj/vondrick4/naveen/HF_CACHE_DIR', load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, num_scheduler_steps=1, chunked_prefill_enabled=True multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "WARNING 11-06 08:51:01 multiproc_gpu_executor.py:53] Reducing Torch parallelism from 255 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 11-06 08:51:01 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1336843)\u001b[0;0m INFO 11-06 08:51:02 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1336842)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1336844)\u001b[0;0m INFO 11-06 08:51:02 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "INFO 11-06 08:51:02 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W1106 08:51:04.351030894 socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [::ffff:127.0.0.1]:41849 (errno: 97 - Address family not supported by protocol).\n",
      "[W1106 08:51:04.653093057 socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [::ffff:127.0.0.1]:41849 (errno: 97 - Address family not supported by protocol).\n",
      "[W1106 08:51:04.653240387 socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [::ffff:127.0.0.1]:41849 (errno: 97 - Address family not supported by protocol).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-06 08:51:05 utils.py:1008] Found nccl from library libnccl.so.2\n",
      "INFO 11-06 08:51:05 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1336844)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1336842)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1336843)\u001b[0;0m INFO 11-06 08:51:05 utils.py:1008] Found nccl from library libnccl.so.2\n",
      "INFO 11-06 08:51:05 utils.py:1008] Found nccl from library libnccl.so.2\n",
      "INFO 11-06 08:51:05 utils.py:1008] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1336844)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1336842)\u001b[0;0m INFO 11-06 08:51:05 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1336843)\u001b[0;0m INFO 11-06 08:51:05 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 11-06 08:51:05 pynccl.py:63] vLLM is using nccl==2.20.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W1106 08:51:04.894772319 socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [::ffff:127.0.0.1]:41849 (errno: 97 - Address family not supported by protocol).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-06 08:51:05 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1336842)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1336844)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1336843)\u001b[0;0m WARNING 11-06 08:51:05 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 11-06 08:51:05 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 11-06 08:51:05 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "INFO 11-06 08:51:05 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7f1ab8c4f190>, local_subscribe_port=59195, remote_subscribe_port=None)\n",
      "INFO 11-06 08:51:05 model_runner.py:1056] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1336842)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1336843)\u001b[0;0m INFO 11-06 08:51:05 model_runner.py:1056] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...\n",
      "INFO 11-06 08:51:05 model_runner.py:1056] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1336844)\u001b[0;0m INFO 11-06 08:51:05 model_runner.py:1056] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1336842)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1336843)\u001b[0;0m INFO 11-06 08:51:06 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "INFO 11-06 08:51:06 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "INFO 11-06 08:51:06 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1336844)\u001b[0;0m INFO 11-06 08:51:06 weight_utils.py:243] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  2.79it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:00<00:00,  2.97it/s]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:00<00:00,  4.17it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:01<00:00,  3.88it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:01<00:00,  3.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-06 08:51:07 model_runner.py:1067] Loading model weights took 3.7710 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=1336843)\u001b[0;0m INFO 11-06 08:51:07 model_runner.py:1067] Loading model weights took 3.7710 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1336844)\u001b[0;0m INFO 11-06 08:51:07 model_runner.py:1067] Loading model weights took 3.7710 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1336842)\u001b[0;0m INFO 11-06 08:51:07 model_runner.py:1067] Loading model weights took 3.7710 GB\n",
      "INFO 11-06 08:51:08 distributed_gpu_executor.py:57] # GPU blocks: 76065, # CPU blocks: 8192\n",
      "INFO 11-06 08:51:08 distributed_gpu_executor.py:61] Maximum concurrency for 131072 tokens per request: 9.29x\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1336842)\u001b[0;0m INFO 11-06 08:51:10 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1336842)\u001b[0;0m INFO 11-06 08:51:10 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1336843)\u001b[0;0m INFO 11-06 08:51:10 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1336843)\u001b[0;0m INFO 11-06 08:51:10 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 11-06 08:51:10 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 11-06 08:51:10 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1336844)\u001b[0;0m INFO 11-06 08:51:10 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1336844)\u001b[0;0m INFO 11-06 08:51:10 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1336844)\u001b[0;0m INFO 11-06 08:51:21 model_runner.py:1523] Graph capturing finished in 11 secs.\n",
      "INFO 11-06 08:51:21 model_runner.py:1523] Graph capturing finished in 11 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1336842)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1336843)\u001b[0;0m INFO 11-06 08:51:21 model_runner.py:1523] Graph capturing finished in 11 secs.\n",
      "INFO 11-06 08:51:21 model_runner.py:1523] Graph capturing finished in 11 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1336844)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1336843)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1336842)\u001b[0;0m INFO 11-06 08:53:20 multiproc_worker_utils.py:240] Worker exiting\n",
      "INFO 11-06 08:53:20 multiproc_worker_utils.py:240] Worker exiting\n",
      "INFO 11-06 08:53:20 multiproc_worker_utils.py:240] Worker exiting\n"
     ]
    }
   ],
   "source": [
    "model_name = 'meta-llama/Meta-Llama-3.1-8B-Instruct'\n",
    "llm = LLM(\n",
    "    model=model_name, \n",
    "    tensor_parallel_size=4, \n",
    "    download_dir=cache_dir,\n",
    "    dtype='bfloat16'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d3e0ec1-1649-4ac9-a188-40ffa8c54e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(\n",
    "    temperature=1.0,\n",
    "    max_tokens=3000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc0ab3a-fcf3-41ee-aed0-cfbb9f98af4f",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3e0083e-d2e1-42ed-8f35-3a560c789490",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.load(open('/proj/vondrick4/naveen/MMTRE/metadata/test_data_filtered_indexed_ultimate_11080(498).json', 'r'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5788a8-05e5-4187-b4ba-2c8200ea5283",
   "metadata": {},
   "source": [
    "## Process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "235a8599-4ad2-4c5a-bb58-d2f8672ca462",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversations = []\n",
    "tracker = {}\n",
    "i = 0\n",
    "\n",
    "for sample in data:\n",
    "    tracker[sample['id']] = {}\n",
    "    instructions = get_instruction(sample['comp'], sample['event_2'])\n",
    "    \n",
    "    conversation_before = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful assistant\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": instructions['before'],\n",
    "        },\n",
    "    ]\n",
    "    conversation_after = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful assistant\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": instructions['after'],\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    conversations.append(conversation_before)\n",
    "    tracker[sample['id']]['before'] = i\n",
    "    i+=1\n",
    "    \n",
    "    conversations.append(conversation_after)\n",
    "    tracker[sample['id']]['after'] = i\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c27bf9-426b-4633-98e5-8c666e1f56af",
   "metadata": {},
   "source": [
    "## Run vLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19e75c0f-8629-464d-9cb4-bcf9cb51073a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  13%|█████████                                                             | 2851/22160 [01:49<16:08, 19.94it/s, est. speed input: 3736.85 toks/s, output: 3287.79 toks/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconversations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43msampling_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msampling_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_tqdm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m      5\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/proj/vondrick2/naveen/miniconda3/envs/tr-vllm/lib/python3.10/site-packages/vllm/entrypoints/llm.py:571\u001b[0m, in \u001b[0;36mLLM.chat\u001b[0;34m(self, messages, sampling_params, use_tqdm, lora_request, chat_template, add_generation_prompt, continue_final_message, tools, mm_processor_kwargs)\u001b[0m\n\u001b[1;32m    567\u001b[0m         prompt[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmm_processor_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m mm_processor_kwargs\n\u001b[1;32m    569\u001b[0m     prompts\u001b[38;5;241m.\u001b[39mappend(prompt)\n\u001b[0;32m--> 571\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    572\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    573\u001b[0m \u001b[43m    \u001b[49m\u001b[43msampling_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msampling_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_tqdm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_tqdm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    575\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlora_request\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlora_request\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/proj/vondrick2/naveen/miniconda3/envs/tr-vllm/lib/python3.10/site-packages/vllm/utils.py:1063\u001b[0m, in \u001b[0;36mdeprecate_kwargs.<locals>.wrapper.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1056\u001b[0m             msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00madditional_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1058\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1059\u001b[0m             \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m(msg),\n\u001b[1;32m   1060\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,  \u001b[38;5;66;03m# The inner function takes up one level\u001b[39;00m\n\u001b[1;32m   1061\u001b[0m         )\n\u001b[0;32m-> 1063\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/proj/vondrick2/naveen/miniconda3/envs/tr-vllm/lib/python3.10/site-packages/vllm/entrypoints/llm.py:353\u001b[0m, in \u001b[0;36mLLM.generate\u001b[0;34m(self, prompts, sampling_params, prompt_token_ids, use_tqdm, lora_request, prompt_adapter_request, guided_options_request, priority)\u001b[0m\n\u001b[1;32m    343\u001b[0m     sampling_params \u001b[38;5;241m=\u001b[39m SamplingParams()\n\u001b[1;32m    345\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_and_add_requests(\n\u001b[1;32m    346\u001b[0m     prompts\u001b[38;5;241m=\u001b[39mparsed_prompts,\n\u001b[1;32m    347\u001b[0m     params\u001b[38;5;241m=\u001b[39msampling_params,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    350\u001b[0m     guided_options\u001b[38;5;241m=\u001b[39mguided_options_request,\n\u001b[1;32m    351\u001b[0m     priority\u001b[38;5;241m=\u001b[39mpriority)\n\u001b[0;32m--> 353\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43muse_tqdm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_tqdm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMEngine\u001b[38;5;241m.\u001b[39mvalidate_outputs(outputs, RequestOutput)\n",
      "File \u001b[0;32m/proj/vondrick2/naveen/miniconda3/envs/tr-vllm/lib/python3.10/site-packages/vllm/entrypoints/llm.py:879\u001b[0m, in \u001b[0;36mLLM._run_engine\u001b[0;34m(self, use_tqdm)\u001b[0m\n\u001b[1;32m    877\u001b[0m total_out_toks \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    878\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_engine\u001b[38;5;241m.\u001b[39mhas_unfinished_requests():\n\u001b[0;32m--> 879\u001b[0m     step_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m step_outputs:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m output\u001b[38;5;241m.\u001b[39mfinished:\n",
      "File \u001b[0;32m/proj/vondrick2/naveen/miniconda3/envs/tr-vllm/lib/python3.10/site-packages/vllm/engine/llm_engine.py:1342\u001b[0m, in \u001b[0;36mLLMEngine.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1335\u001b[0m \u001b[38;5;66;03m# Skip the scheduler if there are any remaining steps in the seq groups.\u001b[39;00m\n\u001b[1;32m   1336\u001b[0m \u001b[38;5;66;03m# This ensures that the scheduler is only called again when the current\u001b[39;00m\n\u001b[1;32m   1337\u001b[0m \u001b[38;5;66;03m# batch has completed.\u001b[39;00m\n\u001b[1;32m   1338\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_remaining_steps(seq_group_metadata_list):\n\u001b[1;32m   1339\u001b[0m     \u001b[38;5;66;03m# Schedule iteration\u001b[39;00m\n\u001b[1;32m   1340\u001b[0m     (seq_group_metadata_list, scheduler_outputs,\n\u001b[1;32m   1341\u001b[0m      allow_async_output_proc\n\u001b[0;32m-> 1342\u001b[0m      ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m[\u001b[49m\u001b[43mvirtual_engine\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1344\u001b[0m     ctx\u001b[38;5;241m.\u001b[39mseq_group_metadata_list \u001b[38;5;241m=\u001b[39m seq_group_metadata_list\n\u001b[1;32m   1345\u001b[0m     ctx\u001b[38;5;241m.\u001b[39mscheduler_outputs \u001b[38;5;241m=\u001b[39m scheduler_outputs\n",
      "File \u001b[0;32m/proj/vondrick2/naveen/miniconda3/envs/tr-vllm/lib/python3.10/site-packages/vllm/core/scheduler.py:1219\u001b[0m, in \u001b[0;36mScheduler.schedule\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1211\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mschedule\u001b[39m(\n\u001b[1;32m   1212\u001b[0m         \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m   1213\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[List[SequenceGroupMetadata], SchedulerOutputs, \u001b[38;5;28mbool\u001b[39m]:\n\u001b[1;32m   1214\u001b[0m     \u001b[38;5;66;03m# Schedule sequence groups.\u001b[39;00m\n\u001b[1;32m   1215\u001b[0m     \u001b[38;5;66;03m# This function call changes the internal states of the scheduler\u001b[39;00m\n\u001b[1;32m   1216\u001b[0m     \u001b[38;5;66;03m# such as self.running, self.swapped, and self.waiting.\u001b[39;00m\n\u001b[1;32m   1217\u001b[0m     scheduler_start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[0;32m-> 1219\u001b[0m     scheduler_outputs: SchedulerOutputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_schedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1220\u001b[0m     now \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   1222\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_config\u001b[38;5;241m.\u001b[39menable_prefix_caching:\n",
      "File \u001b[0;32m/proj/vondrick2/naveen/miniconda3/envs/tr-vllm/lib/python3.10/site-packages/vllm/core/scheduler.py:1176\u001b[0m, in \u001b[0;36mScheduler._schedule\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1174\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Schedule queued requests.\"\"\"\u001b[39;00m\n\u001b[1;32m   1175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler_config\u001b[38;5;241m.\u001b[39mchunked_prefill_enabled:\n\u001b[0;32m-> 1176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_schedule_chunked_prefill\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_schedule_default()\n",
      "File \u001b[0;32m/proj/vondrick2/naveen/miniconda3/envs/tr-vllm/lib/python3.10/site-packages/vllm/core/scheduler.py:1113\u001b[0m, in \u001b[0;36mScheduler._schedule_chunked_prefill\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1110\u001b[0m swapped_in \u001b[38;5;241m=\u001b[39m SchedulerSwappedInOutputs\u001b[38;5;241m.\u001b[39mcreate_empty()\n\u001b[1;32m   1112\u001b[0m \u001b[38;5;66;03m# Decoding should be always scheduled first by fcfs.\u001b[39;00m\n\u001b[0;32m-> 1113\u001b[0m running_scheduled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_schedule_running\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbudget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1114\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43mcurr_loras\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1115\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43menable_chunking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1117\u001b[0m \u001b[38;5;66;03m# Schedule swapped out requests.\u001b[39;00m\n\u001b[1;32m   1118\u001b[0m \u001b[38;5;66;03m# If preemption happens, it means we don't have space for swap-in.\u001b[39;00m\n\u001b[1;32m   1119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(running_scheduled\u001b[38;5;241m.\u001b[39mpreempted) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(\n\u001b[1;32m   1120\u001b[0m         running_scheduled\u001b[38;5;241m.\u001b[39mswapped_out) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/proj/vondrick2/naveen/miniconda3/envs/tr-vllm/lib/python3.10/site-packages/vllm/core/scheduler.py:636\u001b[0m, in \u001b[0;36mScheduler._schedule_running\u001b[0;34m(self, budget, curr_loras, enable_chunking)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;66;03m# OPTIMIZATION:  Note that get_max_num_running_seqs is\u001b[39;00m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;66;03m# expensive. For the default scheduling chase where\u001b[39;00m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;66;03m# enable_chunking is False, num_seqs are updated before running\u001b[39;00m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;66;03m# this method, so we don't have to update it again here.\u001b[39;00m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m enable_chunking:\n\u001b[0;32m--> 636\u001b[0m     num_running_seqs \u001b[38;5;241m=\u001b[39m \u001b[43mseq_group\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_max_num_running_seqs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    637\u001b[0m     budget\u001b[38;5;241m.\u001b[39madd_num_seqs(seq_group\u001b[38;5;241m.\u001b[39mrequest_id, num_running_seqs)\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m curr_loras \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m seq_group\u001b[38;5;241m.\u001b[39mlora_int_id \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/proj/vondrick2/naveen/miniconda3/envs/tr-vllm/lib/python3.10/site-packages/vllm/sequence.py:839\u001b[0m, in \u001b[0;36mSequenceGroup.get_max_num_running_seqs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    836\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m n\n\u001b[1;32m    837\u001b[0m \u001b[38;5;66;03m# At sampling stages, return the number of actual sequences\u001b[39;00m\n\u001b[1;32m    838\u001b[0m \u001b[38;5;66;03m# that are not finished yet.\u001b[39;00m\n\u001b[0;32m--> 839\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_unfinished_seqs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/proj/vondrick2/naveen/miniconda3/envs/tr-vllm/lib/python3.10/site-packages/vllm/sequence.py:897\u001b[0m, in \u001b[0;36mSequenceGroup.num_unfinished_seqs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    895\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnum_unfinished_seqs\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m    896\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_single_seq:\n\u001b[0;32m--> 897\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseqs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_finished\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_unfinished_seqs())\n",
      "File \u001b[0;32m/proj/vondrick2/naveen/miniconda3/envs/tr-vllm/lib/python3.10/site-packages/vllm/sequence.py:608\u001b[0m, in \u001b[0;36mSequence.is_finished\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_finished\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[0;32m--> 608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSequenceStatus\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_finished\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/proj/vondrick2/naveen/miniconda3/envs/tr-vllm/lib/python3.10/site-packages/vllm/sequence.py:73\u001b[0m, in \u001b[0;36mSequenceStatus.is_finished\u001b[0;34m(status)\u001b[0m\n\u001b[1;32m     70\u001b[0m FINISHED_ABORTED \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[1;32m     71\u001b[0m FINISHED_IGNORED \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m6\u001b[39m\n\u001b[0;32m---> 73\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_finished\u001b[39m(status: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSequenceStatus\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m status \u001b[38;5;241m>\u001b[39m SequenceStatus\u001b[38;5;241m.\u001b[39mSWAPPED\n\u001b[1;32m     77\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_finished_reason\u001b[39m(status: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSequenceStatus\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m]:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-06 08:53:22 multiproc_worker_utils.py:120] Killing local vLLM worker processes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  13%|█████████                                                             | 2851/22160 [02:00<16:08, 19.94it/s, est. speed input: 3736.85 toks/s, output: 3287.79 toks/s]"
     ]
    }
   ],
   "source": [
    "outputs = llm.chat(\n",
    "    messages=conversations,\n",
    "    sampling_params=sampling_params,\n",
    "    use_tqdm=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e02ebb-ff54-46ff-ad4a-4dc0086d53f4",
   "metadata": {},
   "source": [
    "## Process & Write Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b9aa5d-eeb1-494c-9269-32ad8937f3d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "unclear = 0\n",
    "processed_outputs = []\n",
    "for out in outputs:\n",
    "    processed_outputs.append(process_llm_outs(out.outputs[0].text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f68aeb6-afbb-4ff3-b9ac-a8d87d0cbb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in data:\n",
    "    sample['possible_events_before'] = processed_outputs[tracker[sample['id']]['before']]\n",
    "    sample['possible_events_after'] = processed_outputs[tracker[sample['id']]['after']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c59108-b9d4-4d4c-b30d-e188415f0469",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/proj/vondrick4/naveen/MMTRE/metadata/zs_data/zero_shot_data_llama_3.1(temp_1.0).json', 'w') as json_file:\n",
    "    json.dump(data, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b2dc48-2a06-49f1-adf9-5ebf5da3fca7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd849eba-360b-4423-9df6-1d352d6e09d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "processed_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927cac36-872e-40ad-8022-96645dccc8a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f671b623-ef97-4900-ad62-802703ce5a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "100*unclear/len(processed_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f332fa2-ee15-42be-85e2-99b16922066c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b5d886-60d7-487d-b77a-7a29696803fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f3a9d9-b95c-4467-826b-05df0900fa0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ee7e95-cded-4af5-9ec0-50acd282d1fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31f1238-0c3f-4350-81e1-f6686191320d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f89bce-4590-47d4-9b24-ae43539b054f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
