{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f963625-2675-4e2b-b7f0-07fd32186128",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eeecda67-c727-46aa-abb4-39b1f294f643",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/proj/vondrick2/naveen/miniconda3/envs/tr-vllm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-11-06 10:15:37,383\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "import tqdm as notebook_tqdm\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import copy\n",
    "from huggingface_hub import login\n",
    "\n",
    "with open('/home/nd2794/keys.json', 'r') as file:\n",
    "    keys = json.load(file)\n",
    "\n",
    "cache_dir = '/proj/vondrick4/naveen/HF_CACHE_DIR'\n",
    "hf_token = keys[\"HF_READ_TOKEN\"]\n",
    "login(token=hf_token)\n",
    "\n",
    "os.environ['TRANSFORMERS_CACHE'] = cache_dir\n",
    "os.environ['HUGGINGFACE_HUB_CACHE'] = cache_dir\n",
    "os.environ['PYTORCH_TRANSFORMERS_CACHE'] = cache_dir\n",
    "os.environ['PYTORCH_PRETRAINED_BERT_CACHE'] = cache_dir\n",
    "os.environ['HF_HUB_CACHE'] = cache_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d808f10-d4fc-4061-93ed-d0a33d0f60fc",
   "metadata": {},
   "source": [
    "## Define Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fab017b-cad8-442b-8a6e-d65f81e29576",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_instruction(source_event, target_event):\n",
    "    common_instruct = \"\"\"Two images (Source & Target) are described by their captions.\n",
    "Your have generate a query text for composed image retrieval task that describes how the Source image should be modified to get the Target image.\n",
    "Each image may multiple captions which are listed in a new line. You must read all the captions for each image to fully understand the image.\n",
    "Only output the query text.\"\"\"\n",
    "\n",
    "    source_instruct = \"\"\"SOURCE IMAGE CAPTIONS:\\n\"\"\" + \"\\n\".join(f\"- {item}\" for item in source_event)\n",
    "\n",
    "    target_instruct = \"\"\"TARGET IMAGE CAPTIONS:\\n\"\"\" + \"\\n\".join(f\"- {item}\" for item in target_event)\n",
    "\n",
    "    return common_instruct + '\\n\\n' + source_instruct + '\\n\\n' + target_instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "898201d9-5cbd-4cf0-b527-326500c038c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_source_cap = [\n",
    "    'A person in blue and orange shirt riding a bicycle.',\n",
    "    'A bicyclist riding in front of speed blurred foliage. ',\n",
    "    'A cyclist with a helmet bicycles quickly on a road.',\n",
    "    'The man is riding his bike down the street. '\n",
    "]\n",
    "\n",
    "ex_target_cap = [\n",
    "    'A man is riding a motorcycle down the street',\n",
    "    'A man riding a motorcycle down a street next to a restaurant.',\n",
    "    'Helmeted motorcyclist riding on roadway in populated setting. ',\n",
    "    'A motorcycle going down a street very fast.'\n",
    "]\n",
    "\n",
    "ex_query = \"The person is riding a motorcycle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b36e1b7c-aac7-4428-9942-bfefd3590d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_template = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful assistant\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": get_instruction(ex_source_cap, ex_target_cap)\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": ex_query \n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0702d8-9170-4e71-ba6a-4a534e24a80b",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27365b48-1d3c-476d-bbfb-2eac0e734b98",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-06 10:15:48 config.py:905] Defaulting to use mp for distributed inference\n",
      "WARNING 11-06 10:15:48 arg_utils.py:957] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.\n",
      "INFO 11-06 10:15:48 config.py:1021] Chunked prefill is enabled with max_num_batched_tokens=512.\n",
      "INFO 11-06 10:15:48 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir='/proj/vondrick4/naveen/HF_CACHE_DIR', load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, num_scheduler_steps=1, chunked_prefill_enabled=True multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "WARNING 11-06 10:15:48 multiproc_gpu_executor.py:53] Reducing Torch parallelism from 255 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 11-06 10:15:48 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1392604)\u001b[0;0m INFO 11-06 10:15:49 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1392605)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1392606)\u001b[0;0m INFO 11-06 10:15:49 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "INFO 11-06 10:15:49 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W1106 10:15:51.984727320 socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [::ffff:127.0.0.1]:35777 (errno: 97 - Address family not supported by protocol).\n",
      "[W1106 10:15:51.432841821 socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [::ffff:127.0.0.1]:35777 (errno: 97 - Address family not supported by protocol).\n",
      "[W1106 10:15:51.464829314 socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [::ffff:127.0.0.1]:35777 (errno: 97 - Address family not supported by protocol).\n",
      "[W1106 10:15:51.632998437 socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [::ffff:127.0.0.1]:35777 (errno: 97 - Address family not supported by protocol).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-06 10:15:51 utils.py:1008] Found nccl from library libnccl.so.2\n",
      "INFO 11-06 10:15:51 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1392604)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1392605)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1392606)\u001b[0;0m INFO 11-06 10:15:51 utils.py:1008] Found nccl from library libnccl.so.2\n",
      "INFO 11-06 10:15:51 utils.py:1008] Found nccl from library libnccl.so.2\n",
      "INFO 11-06 10:15:51 utils.py:1008] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1392604)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1392606)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1392605)\u001b[0;0m INFO 11-06 10:15:51 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 11-06 10:15:51 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 11-06 10:15:51 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "WARNING 11-06 10:15:52 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1392605)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1392604)\u001b[0;0m WARNING 11-06 10:15:52 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 11-06 10:15:52 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1392606)\u001b[0;0m WARNING 11-06 10:15:52 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "INFO 11-06 10:15:52 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7fc604c46080>, local_subscribe_port=57055, remote_subscribe_port=None)\n",
      "INFO 11-06 10:15:52 model_runner.py:1056] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1392604)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1392605)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1392606)\u001b[0;0m INFO 11-06 10:15:52 model_runner.py:1056] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...\n",
      "INFO 11-06 10:15:52 model_runner.py:1056] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...\n",
      "INFO 11-06 10:15:52 model_runner.py:1056] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...\n",
      "INFO 11-06 10:15:52 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1392605)\u001b[0;0m INFO 11-06 10:15:52 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1392604)\u001b[0;0m INFO 11-06 10:15:52 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1392606)\u001b[0;0m INFO 11-06 10:15:52 weight_utils.py:243] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:00,  3.39it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:00<00:00,  3.32it/s]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:00<00:00,  4.55it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:01<00:00,  3.61it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:01<00:00,  3.67it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-06 10:15:54 model_runner.py:1067] Loading model weights took 3.7710 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1392604)\u001b[0;0m INFO 11-06 10:15:54 model_runner.py:1067] Loading model weights took 3.7710 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1392606)\u001b[0;0m INFO 11-06 10:15:54 model_runner.py:1067] Loading model weights took 3.7710 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1392605)\u001b[0;0m INFO 11-06 10:15:54 model_runner.py:1067] Loading model weights took 3.7710 GB\n",
      "INFO 11-06 10:15:55 distributed_gpu_executor.py:57] # GPU blocks: 76065, # CPU blocks: 8192\n",
      "INFO 11-06 10:15:55 distributed_gpu_executor.py:61] Maximum concurrency for 131072 tokens per request: 9.29x\n",
      "INFO 11-06 10:15:57 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 11-06 10:15:57 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1392606)\u001b[0;0m INFO 11-06 10:15:57 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1392606)\u001b[0;0m INFO 11-06 10:15:57 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1392604)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1392605)\u001b[0;0m INFO 11-06 10:15:57 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 11-06 10:15:57 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1392604)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1392605)\u001b[0;0m INFO 11-06 10:15:57 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 11-06 10:15:57 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1392606)\u001b[0;0m INFO 11-06 10:16:10 model_runner.py:1523] Graph capturing finished in 13 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1392604)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1392605)\u001b[0;0m INFO 11-06 10:16:10 model_runner.py:1523] Graph capturing finished in 13 secs.\n",
      "INFO 11-06 10:16:10 model_runner.py:1523] Graph capturing finished in 13 secs.\n",
      "INFO 11-06 10:16:10 model_runner.py:1523] Graph capturing finished in 13 secs.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'meta-llama/Meta-Llama-3.1-8B-Instruct'\n",
    "llm = LLM(\n",
    "    model=model_name, \n",
    "    tensor_parallel_size=4, \n",
    "    download_dir=cache_dir,\n",
    "    dtype='bfloat16'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e15a6f31-18cb-478a-8bc0-dcd7d79f7d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(\n",
    "    #temperature=1.0,\n",
    "    max_tokens=200,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb10ae37-01a2-4150-8dd8-811410b0a42b",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c0eb3dca-9a70-4144-ac24-f7275c4c2424",
   "metadata": {},
   "outputs": [],
   "source": [
    "split = 'train'\n",
    "if split == 'train':\n",
    "    data = json.load(open('/proj/vondrick4/naveen/coir-data/LaSCo/metadata/lasco_train_indexed_MT(img_cap).json', 'r'))\n",
    "    imgs_and_caps = json.load(open('/proj/vondrick4/naveen/coir-data/annotations/images_and_caps_train2014_processed_dict.json', 'r'))\n",
    "elif split == 'val':\n",
    "    data = json.load(open('/proj/vondrick4/naveen/coir-data/LaSCo/metadata/lasco_val_indexed_MT(img_cap).json', 'r'))\n",
    "    imgs_and_caps= json.load(open('/proj/vondrick4/naveen/coir-data/annotations/images_and_caps_val2014_processed_dict.json', 'r'))\n",
    "    for key in imgs_and_caps.keys():\n",
    "        imgs_and_caps[key] = [imgs_and_caps[key]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122c3f3b-a542-4c77-b246-a88f69761899",
   "metadata": {},
   "source": [
    "## Process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "860b9541-2e2b-44df-9b34-5dd6f3bd250d",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversations = []\n",
    "tracker = {}\n",
    "i = 0\n",
    "\n",
    "for sample in data:\n",
    "    tracker[sample['id']] = {}\n",
    "    \n",
    "    conversation_forward = copy.deepcopy(conversation_template)\n",
    "    conversation_forward.append(\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": get_instruction(imgs_and_caps[str(sample[\"coir\"][\"query-image-id\"])], imgs_and_caps[str(sample[\"coir\"][\"target-image-id\"])])\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    conversation_reverse = copy.deepcopy(conversation_template)\n",
    "    conversation_reverse.append(\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": get_instruction(imgs_and_caps[str(sample[\"coir\"][\"target-image-id\"])], imgs_and_caps[str(sample[\"coir\"][\"query-image-id\"])])\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    conversations.append(conversation_forward)\n",
    "    tracker[sample['id']]['forward'] = i\n",
    "    i+=1\n",
    "    \n",
    "    conversations.append(conversation_reverse)\n",
    "    tracker[sample['id']]['reverse'] = i\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d945fbd3-8fd3-41cf-a34e-fa1c8a6a2455",
   "metadata": {},
   "source": [
    "## Run vLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f30f461e-469a-43ce-8848-244ee7e128d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████████████████████████████████████████████████████| 200/200 [00:07<00:00, 26.59it/s, est. speed input: 9340.37 toks/s, output: 445.31 toks/s]\n"
     ]
    }
   ],
   "source": [
    "outputs = llm.chat(\n",
    "    messages=conversations,\n",
    "    sampling_params=sampling_params,\n",
    "    use_tqdm=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65041fc4-f980-447f-b9d9-b1f9a71ffaf1",
   "metadata": {},
   "source": [
    "## Process & Write Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4ab3c37f-8f86-488d-83d7-d8440f5ad1d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "processed_outputs = []\n",
    "for out in outputs:\n",
    "    processed_outputs.append(out.outputs[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "02c457cf-294b-4308-835a-7cc952b7a79f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for sample in data:\n",
    "    sample[\"coir\"]['query-text-forward-mg'] = processed_outputs[tracker[sample['id']]['forward']]\n",
    "    sample[\"coir\"]['query-text-reverse-mg'] = processed_outputs[tracker[sample['id']]['reverse']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e139a38b-ed21-48dd-9c5f-82cf9f24265e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR 11-06 10:39:17 multiproc_worker_utils.py:116] Worker VllmWorkerProcess pid 1392604 died, exit code: -15\n",
      "INFO 11-06 10:39:17 multiproc_worker_utils.py:120] Killing local vLLM worker processes\n"
     ]
    }
   ],
   "source": [
    "with open('/proj/vondrick4/naveen/coir-data/LaSCo/metadata/lasco_{}_indexed_MT(img_cap)_MG.json'.format(split), 'w') as json_file:\n",
    "    json.dump(data, json_file, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
