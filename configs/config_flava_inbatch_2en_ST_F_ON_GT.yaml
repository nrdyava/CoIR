exp_name: 'FLAVA_inbatch_2en_ST_F_ON_GTY_LR_1e-5'       # Experiment name
comments: 'Two trainable encoders, single task, only forward, FLAVA model finetuning, Only one normalization after addition, multi-GPU matrix gathered training - enabled'

exp_type: 'flava_inbatch_2en_ST_F_ON_GT'       # choices: ['flava_inbatch_2en_ST_F_ON_GT']
model_type: 'FLAVA'        # choices: ['FLAVA']
task: 'finetune'        # choices: [finetune, continue_training, test_only]
dataset_to_use: 'lasco'       # choices: [lasco, fashioniq, cirr, circo]

#pretrained_checkpoint_path: '/proj/vondrick2/naveen/pretrained_models/clip/clip-vit-base-patch32'
#checkpoint_path: '/local/vondrick/nd2794/pretrained_models/clip/clip-vit-base-patch32'
resume_training_from_checkpoint: '/proj/vondrick4/naveen/coir-runs/{replace-with-suitable-run-name}/{checkpoint}'  # Lightning checkpoint path
#resume_from_checkpoint: None
test_at_checkpoint: '/proj/vondrick4/naveen/coir-runs/{replace-with-suitable-run-name}/{checkpoint}'  # Lightning checkpoint path

image_encoder_mode: 'train' # choices: [train, freeze]
text_encoder_mode: 'train' # choices: [train, freeze]


data_augmentation:
  enable: True

gather_embeddings: True
find_unused_parameters: True

dataloader:
  batch_size: 140
  num_workers: 5
  pin_memory: True
  shuffle:
    train: True
    val: False
    test: False
  persistent_workers: True
  drop_last: False


optimizer:
  lr: 0.00001
  name: 'AdamW'   # choices: ['AdamW']
  warmup_steps: 0
  weight_decay: 0.0
  # LR Scheduler parameters (ConsineAnnealingLR)
  eta_min: 0.0


loss_fn:
  name: 'contrastive_asymetric_loss_with_softmax_temp'
  temperature: 0.01
  train_temperature: True # choices: [True, False]
  #temp_clamp_max: 100

trainer:
  max_epochs: 30
  min_epochs: 1
  check_val_every_n_epoch: 1
  accelerator: 'gpu' 
  strategy: 'ddp' #[deepspeed_stage_3, deepspeed_stage_2, ddp]
  devices: [0, 1, 2, 3, 4, 5, 6, 7] #[0, 1, 2, 3, 4, 5, 6, 7]
  use_distributed_sampler: True
  log_every_n_steps: 20
  enable_checkpointing: True
  fast_dev_run: False
  limit_train_batches: 1.0
  limit_val_batches: 1.0
  limit_test_batches: 0.1
  limit_predict_batches: 0.1
  profiler: 'None' # ['None', 'simple', 'advanced', 'pytorch']
  num_nodes: 1
  precision: '32-true'
  num_sanity_val_steps: 0
  enable_progress_bar: True
  enable_model_summary: True
  deterministic: True
  benchmark: True
  model_checkpoint_callback:
    every_n_epochs: 1
    monitor: 'val_loss'
    mode: 'min'
    save_top_k: -1  # 0 means no saving, -1 means save all.
    filename: 'checkpoint-{epoch:03d}'
    enable_version_counter: False


wandb:
  project: coir

runs_dir: '/proj/vondrick4/naveen/coir-runs'
run_registry_file: '/proj/vondrick4/naveen/coir-run-registry.txt'

# Redundant values for rank != 0
#run_dir: 'redundant_name_for_rank != 0'
#wandb_name: 'redundant_name_for_rank != 0'


data:
  lasco:
    dir: '/local/vondrick/naveen/coir-data/LaSCo'
    splits: ['train', 'val']
  fashioniq:
    dir: '/local/vondrick/naveen/coir-data/FashionIQ'
    splits: ['train', 'val', 'test']
  cirr:
    dir: '/local/vondrick/naveen/coir-data/CIRR'
    splits: ['train', 'val', 'test1']
  circo:
    dir: '/local/vondrick/naveen/coir-data/CIRCO'
    splits: ['val', 'test']


seed: 42
local_time_zone: 'US/Eastern'
TOKENIZERS_PARALLELISM: 'true'    # choices: ['true', 'false']
CUDA_LAUNCH_BLOCKING: '1'
TORCH_USE_CUDA_DS: '1'
float32_matmul_precision: high
