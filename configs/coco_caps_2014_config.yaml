results_file_name: 'coco_2014_dotp_results/CLIP-ViT_B_32_baseline.txt'
dataset_to_use: lasco
dataset_split: 'val'    #options:['train', 'val']

coco_caps_2014_path: '/local/vondrick/nd2794/CoIR/data/coco_2014_annotations'

model_gpu_device_id: 0  # Model will always use GPU for inference

eval_model_type: 'baseline'     # options: ['baseline', 'fine_tuned']
d: 512  # Embedding dimension

image_encoder_mode: 'train' # choices: [train, freeze]
text_encoder_mode: 'train' # choices: [train, freeze]

optimizer:
  lr: 0.00005
  name: 'adamw'
  #weight_decay: 0.1
  #betas: [0.9, 0.999]
  #eps: 1e-8

# Lightning Checkpoint to use for inference on fine-tuned models. Give a dummy path while evaluating baseline models
pl_ckpt_path: '/local/vondrick/nd2794/CoIR/runs/2024-09-01-05-18-16-241590/checkpoint-epoch=04-val_loss=1.1510791779.ckpt'
# CLIP pre-trained model to be used for inference
checkpoint_path: '/local/vondrick/nd2794/pretrained_models/clip/clip-vit-base-patch32'

dataloader:
  batch_size: 100
  num_workers: 20
  pin_memory: True
  shuffle: False
  persistent_workers: True
  drop_last: False


seed: 42
float32_matmul_precision: high


data:
  lasco:
    dir: '/local/vondrick/nd2794/CoIR/data/LaSCo'
    img_tensors_dir: '/local/vondrick/nd2794/CoIR/data/LaSCo_img_tensors'
    splits: ['train', 'val']
  fashioniq:
    dir: '/local/vondrick/nd2794/CoIR/data/FashionIQ'
    splits: ['train', 'val', 'test']
  cirr:
    dir: '/local/vondrick/nd2794/CoIR/data/CIRR'
    splits: ['train', 'val', 'test1']
  circo:
    dir: '/local/vondrick/nd2794/CoIR/data/CIRCO'
    splits: ['val', 'test']
  coco_2014_annotations:
    dir: '/local/vondrick/nd2794/CoIR/data/coco_2014_annotations'
    splits: ['val', 'train']


TOKENIZERS_PARALLELISM: 'true'    # choices: ['true', 'false']
CUDA_LAUNCH_BLOCKING: '1'
TORCH_USE_CUDA_DS: '1'


loss_fn:
  name: 'symmetric_loss_with_temp'
  temperature: 0.07
  train_temperature: True # choices: [True, False]
  #temp_clamp_max: 100
  dotp_clip: '1234'